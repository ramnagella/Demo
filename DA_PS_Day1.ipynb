{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramnagella/Demo/blob/master/DA_PS_Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DA with PySpark 30-08-2024--\n",
        "#Installing PySpark ---https://medium.com/@uzzaman.ahmed/setup-pyspark-on-local-machine-and-link-it-with-jupyter-notebook-windows-e08349e792db\n",
        "#Check the concepts of hadoop,bigdata and Spark\n",
        "#Structured  data --RDBMS--SQL,oracle ,mysql---Client/Server arch\n",
        "#Semi Structured----XML,JSON,email,text\n",
        "#un-strcurued -----audio,video,pdf's ,images,animated pictures--hadoop is master/slave arch--multiple Master(controller) nodes and slave nodes\n",
        "#no-sql---mangodb,cassandra,hbase\n",
        "#Any data(SD/SSD/USD) which is beyond the storage and processing capabilities of RDBMS\n",
        "#only 20% data is SD rest 80 % data is SSD and USD\n",
        "#GB means better to use RDBMS ---PB and TB means Bid data -Twitter 30 TB /Month\n",
        "#Velocity means the rate at which data is genrated ---is data mb/sec means RDBMS--GB or TB/Sec means big data\n",
        "#Verocity how far data is reliable ---RDBMS --only SD ---Big data --accepts any type of data\n",
        "#haddop is framework developed by apache opensource\n",
        "#Hadoop horizontal scaling --supports parallel processing and distributed storage --map/reduce programming\n",
        "#Drawback of hadoop is ---hadoop supports batch processing  and HDFS---storage and processing happens in Hard disk\n",
        "#In real time it is difficult--hadoop doesnt support realtime processing of data\n",
        "#Real time data processing --in memory data processing and data processing applications ---master/slave -SPARK better choice -- RDD ---\n",
        "#in SPARK initial read and write happens in hard disk --further reads and writes in memory of the data nodes\n",
        "#intermediate R/W happens from ---RAM --and final writes happens into Hard disk\n",
        "#in real we can cal spark is advanced one of hadoop\n",
        "#Spark supports Resilient distribution dataset---RDD"
      ],
      "metadata": {
        "id": "E33s6I6zRB1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to install java Version ---to install pyspark we need install JDK 1.8---then check pyspark version\n",
        "#conda install -c conda-forge openjdk\n",
        "#conda install conda-forge::pyspark\n",
        "#conda activate pyspark_new\n",
        "#pip show pyspark\n",
        "#conda install -c conda-forge findspark---to initiate the spark image"
      ],
      "metadata": {
        "id": "uzEwYjb8RBxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2-09-2024----Spark and PySpark---\n",
        "#Spark Architecture is as below ---Client server executes the Driver program (jar file program),in master node--worker node--cluster manager\n",
        "#1 GB allocated to worker nodes ---Resource allocation taken care by cluster manager(hadoop framework)--it is component of haddop (clsternamanger and driver program) --like splitting the data\n",
        "# and allocating all the worker nodes --W1,W2,W3.....after the allocation all these worker nodes register into master nodes\n",
        "#Master nodes communicates with worker nodes using heart beat ---worker nodes are executor---\n",
        "#Only resource allocation purpose will go for cluster manager\n",
        "#Master nodes worker nodes aee not virtual machones resources are allocated by cluster manager for the hadoop components.\n",
        "#one worker node act as active master node and another worker acta s passive master node if primary master node failes\n",
        "#secondary master node will take over the tasks.----zookeeper ---ZooKeeper work on the election process to elect one worker node as to act as master node\n",
        "#"
      ],
      "metadata": {
        "id": "opreKqO0RBvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' PySpark Architecture----Client go with ----> Driver Program initiated called master node will create automatically initially SparkContext object\n",
        "----SparkContext is entry point for spark functionality---one SC is created for JVM ---\n",
        "----We cant create multiple SC objects first we need to stop or kill active SC object and create new SC object\n",
        "----SparkContext---here we can give app name --or\n",
        "----how much memlory or disk space is used we can write in SC object\n",
        "----SC --->picked file --->py4j virtual machone engine\n",
        "----in PySpark, \"picked file\" primarily refers to files that have been serialized using Python's pickle module and then saved to a distributed file system by Spark.\n",
        "----This process allows for the efficient storage and retrieval of Python objects within a Spark environment\n",
        "----Client--->DriverPRG+SC-->picked file-->coverted into byte stream object->py4j virtual machone engine-->Python RDD-->master node-->Cluster manager-->Worker node\n",
        "----Data Structures are RDD,DF and dataset---PySpark supports RDD and DF omly--RDD is not having 2-D\n",
        "----RDD how data is distributed means suppose ---one data set is like [1,2,3,4,5,6,7,8,910]\n",
        "----one dataset [1,2,3,4] is allocated for one Worker node and rest of the elements are allocated for another worker node\n",
        "----[5,6,7,8,9,10]---DF means contaon rows and columns---low level versions relased RDD--\n",
        "---1.3 above versions relases DF and 1.6 and above dataset\n",
        "---need to install --java 1.8---pyspark---hadoop\n",
        "---set SPARK_HOME=C:\\spark-3.5.2-bin-hadoop3\\spark-3.5.2-bin-hadoop3\n",
        "---set PYSPARK_PYTHON=E:\\anaconda\\python.exe\n",
        "---run pyspark command from command promt then you can see spark env in jupyter notebook and immediately SC object is automatically created\n",
        "---take jupyter notebook URL from command prompt and see in the browser---\n",
        "---in jupyter notebook type the below code and see\n",
        "---\n",
        "\n",
        "----import pyspark\n",
        "----run sc in another jupyter notebook cell and you also run spark means spark session\n",
        "----if you want to work with hive ue spark session other wise we can use SC\n",
        "----if you want to execute SQL related queries SPARK session is necessary.\n",
        "---'''"
      ],
      "metadata": {
        "id": "Wwebiyq7xqU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pysprak.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('practice').getOrCreate()\n",
        "print(spark.version)"
      ],
      "metadata": {
        "id": "ZRf59mNKRBs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "mlSLzXNlRBqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "#initialize Spark Session\n",
        "spark=SparkSession.builder.appName('PySpark Test').getOrCreate()\n",
        "print(spark.version)\n",
        "#Create a Sample DF\n",
        "data =[('Ram',45),('Shatha',13),('Vamsi',17)]\n",
        "columns=['Name','Age']\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-VQMJ9mRBnb",
        "outputId": "2a6bc77e-34a7-423e-951f-442839f8701b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.1\n",
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "|   Ram| 45|\n",
            "|Shatha| 13|\n",
            "| Vamsi| 17|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark\n",
        "#Click on Spark UI in below you will get the spark contect details"
      ],
      "metadata": {
        "id": "iCP5-RTBzwzr",
        "outputId": "0b088736-88ce-4855-bf79-3ddaf65c9ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a1ec4afca10>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://daaa661da5fe:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark Test</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "NdEh94nl5L3N",
        "outputId": "419d0b4b-828a-4fa4-b9a5-6f0c4efc15f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-4134826275.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import local\n",
        "from pyspark import SparkContext\n",
        "sc=SparkContext(master='local[4]',\n",
        "                appName='Sample')\n",
        "\n",
        "#will get error because already SC object was created in previous cells\n",
        "#stop the SC and re-run the above code"
      ],
      "metadata": {
        "id": "kvPBOZN1zwwE",
        "outputId": "5cc36483-b731-41d3-8fe5-36b3fb33efdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySpark Test, master=local[*]) created by getOrCreate at /tmp/ipython-input-1-3384325210.py:3 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-2809705833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthreading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m sc=SparkContext(master='local[4]',\n\u001b[0m\u001b[1;32m      4\u001b[0m                 appName='Sample')\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySpark Test, master=local[*]) created by getOrCreate at /tmp/ipython-input-1-3384325210.py:3 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "GLwSPQgHzwtN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import local\n",
        "from pyspark import SparkContext\n",
        "sc=SparkContext(master='local[4]',\n",
        "                appName='Sample')"
      ],
      "metadata": {
        "id": "cjSfyGhxzwmU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "id": "AE60oTO3zwjE",
        "outputId": "6e964eeb-e63b-432c-e067-0f43a3198cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[4] appName=Sample>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://daaa661da5fe:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[4]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Sample</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD---"
      ],
      "metadata": {
        "id": "iRrFiq7YzweQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "5AAX_Xn_zwcC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "D_9m1HC6OrfB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "sc=SparkContext(master='local[4]')\n",
        "lst=np.random.randint(0,10,20)\n",
        "A=sc.parallelize(lst)\n",
        "print(A)\n",
        "#Here will get parallel collection RDD"
      ],
      "metadata": {
        "id": "MgTIl8bgzwZn",
        "outputId": "78ab9b46-67fe-4598-ed9a-7cfdd139bf85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(lst)"
      ],
      "metadata": {
        "id": "7DXhDuljzwXQ",
        "outputId": "a668f170-f6f5-4e5d-936e-16e8a6b5da63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 4 7 8 0 8 5 9 0 3 5 7 1 8 2 7 4 1 8 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.glom().collect()#glom is transformation and collect is action method"
      ],
      "metadata": {
        "id": "wZJTwUzozwUy",
        "outputId": "fd90e72b-fa78-4547-8d7c-7aa2af0907e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[np.int64(1), np.int64(4), np.int64(7), np.int64(8), np.int64(0)],\n",
              " [np.int64(8), np.int64(5), np.int64(9), np.int64(0), np.int64(3)],\n",
              " [np.int64(5), np.int64(7), np.int64(1), np.int64(8), np.int64(2)],\n",
              " [np.int64(7), np.int64(4), np.int64(1), np.int64(8), np.int64(8)]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.glom()"
      ],
      "metadata": {
        "id": "qK6Zil6JzwSN",
        "outputId": "6d673d71-2043-494c-fe8e-649135ce0d61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[5] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "WR9z2PKTzwQC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "ruHw6U03zvqs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc=SparkContext('local[4]','Shatha')"
      ],
      "metadata": {
        "id": "9d7G4YtbKCWd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession(sc)"
      ],
      "metadata": {
        "id": "UAckxsyEKCS2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "li=[('Java',10),('Python',20),('C',30)]"
      ],
      "metadata": {
        "id": "z4hPe5BtKCQV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize(li)"
      ],
      "metadata": {
        "id": "josvHqsPKCOA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2=sc.textFile('cars.csv')"
      ],
      "metadata": {
        "id": "TJ1hnSzWKCLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.glom()\n",
        "#Here will get PythonRDD object"
      ],
      "metadata": {
        "id": "efGHlk7cPNTj",
        "outputId": "2f5421a6-fd87-4e6b-b9b3-b9cefd8436fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[1] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.glom().collect()"
      ],
      "metadata": {
        "id": "jEkjCU3oPxra",
        "outputId": "36392453-8c8a-489c-da6a-b3e47f29c5d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [('Java', 10)], [('Python', 20)], [('C', 30)]]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(rdd)"
      ],
      "metadata": {
        "id": "ElANV8NqKCI7",
        "outputId": "fac39c1b-f178-40a4-cfac-013a7f464038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.rdd.RDD</b><br/>def __init__(jrdd: &#x27;JavaObject&#x27;, ctx: &#x27;SparkContext&#x27;, jrdd_deserializer: Serializer=AutoBatchedSerializer(CPickleSerializer()))</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py</a>A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
              "Represents an immutable, partitioned collection of elements that can be\n",
              "operated on in parallel.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 336);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark.rdd.RDD"
      ],
      "metadata": {
        "id": "U9aOy7psKCGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.appName('PySpark Test')\\\n",
        ".config('spark.executor.memory','4g')\\#worker node\n",
        ".config('spark.executor.cores','2')\\\n",
        ".config('spark.driver.memory','4g')\\#master node\n",
        ".config('spark.driver.cores','2')\\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "bXX7jpR6LwST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.Transformation\n",
        "2.Actions\n",
        "RDD\n",
        "-----[1,2,3,4,5]==>RDD==>worker node paertition==>\n",
        "-----Transformations--lazy evaluated (sum of list element reduce())\n",
        "-----new RDD doesnt return any value )---Actions (return value)"
      ],
      "metadata": {
        "id": "07yFnAFxLwO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWbL37mOLwMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYBGQWscLwJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UbP7tjBILwHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w34D6lFyLwFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGVa30ECLwCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AX8wdYF0Lv_m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}