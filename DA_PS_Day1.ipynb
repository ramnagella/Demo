{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramnagella/Demo/blob/master/DA_PS_Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DA with PySpark 30-08-2024--\n",
        "#Installing PySpark ---https://medium.com/@uzzaman.ahmed/setup-pyspark-on-local-machine-and-link-it-with-jupyter-notebook-windows-e08349e792db\n",
        "#Check the concepts of hadoop,bigdata and Spark\n",
        "#Structured  data --RDBMS--SQL,oracle ,mysql---Client/Server arch\n",
        "#Semi Structured----XML,JSON,email,text\n",
        "#un-strcurued -----audio,video,pdf's ,images,animated pictures--hadoop is master/slave arch--multiple Master(controller) nodes and slave nodes\n",
        "#no-sql---mangodb,cassandra,hbase\n",
        "#Any data(SD/SSD/USD) which is beyond the storage and processing capabilities of RDBMS\n",
        "#only 20% data is SD rest 80 % data is SSD and USD\n",
        "#GB means better to use RDBMS ---PB and TB means Bid data -Twitter 30 TB /Month\n",
        "#Velocity means the rate at which data is genrated ---is data mb/sec means RDBMS--GB or TB/Sec means big data\n",
        "#Verocity how far data is reliable ---RDBMS --only SD ---Big data --accepts any type of data\n",
        "#haddop is framework developed by apache opensource\n",
        "#Hadoop horizontal scaling --supports parallel processing and distributed storage --map/reduce programming\n",
        "#Drawback of hadoop is ---hadoop supports batch processing  and HDFS---storage and processing happens in Hard disk\n",
        "#In real time it is difficult--hadoop doesnt support realtime processing of data\n",
        "#Real time data processing --in memory data processing and data processing applications ---master/slave -SPARK better choice -- RDD ---\n",
        "#in SPARK initial read and write happens in hard disk --further reads and writes in memory of the data nodes\n",
        "#intermediate R/W happens from ---RAM --and final writes happens into Hard disk\n",
        "#in real we can cal spark is advanced one of hadoop\n",
        "#Spark supports Resilient distribution dataset---RDD"
      ],
      "metadata": {
        "id": "E33s6I6zRB1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to install java Version ---to install pyspark we need install JDK 1.8---then check pyspark version\n",
        "#conda install -c conda-forge openjdk\n",
        "#conda install conda-forge::pyspark\n",
        "#conda activate pyspark_new\n",
        "#pip show pyspark\n",
        "#conda install -c conda-forge findspark---to initiate the spark image"
      ],
      "metadata": {
        "id": "uzEwYjb8RBxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2-09-2024----Spark and PySpark---\n",
        "#Spark Architecture is as below ---Client server executes the Driver program (jar file program),in master node--worker node--cluster manager\n",
        "#1 GB allocated to worker nodes ---Resource allocation taken care by cluster manager(hadoop framework)--it is component of haddop (clsternamanger and driver program) --like splitting the data\n",
        "# and allocating all the worker nodes --W1,W2,W3.....after the allocation all these worker nodes register into master nodes\n",
        "#Master nodes communicates with worker nodes using heart beat ---worker nodes are executor---\n",
        "#Only resource allocation purpose will go for cluster manager\n",
        "#Master nodes worker nodes aee not virtual machones resources are allocated by cluster manager for the hadoop components.\n",
        "#one worker node act as active master node and another worker acta s passive master node if primary master node failes\n",
        "#secondary master node will take over the tasks.----zookeeper ---ZooKeeper work on the election process to elect one worker node as to act as master node\n",
        "#"
      ],
      "metadata": {
        "id": "opreKqO0RBvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' PySpark Architecture----Client go with ----> Driver Program initiated called master node will create automatically initially SparkContext object\n",
        "----SparkContext is entry point for spark functionality---one SC is created for JVM ---\n",
        "----We cant create multiple SC objects first we need to stop or kill active SC object and create new SC object\n",
        "----SparkContext---here we can give app name --or\n",
        "----how much memlory or disk space is used we can write in SC object\n",
        "----SC --->picked file --->py4j virtual machone engine\n",
        "----in PySpark, \"picked file\" primarily refers to files that have been serialized using Python's pickle module and then saved to a distributed file system by Spark.\n",
        "----This process allows for the efficient storage and retrieval of Python objects within a Spark environment\n",
        "----Client--->DriverPRG+SC-->picked file-->coverted into byte stream object->py4j virtual machone engine-->Python RDD-->master node-->Cluster manager-->Worker node\n",
        "----Data Structures are RDD,DF and dataset---PySpark supports RDD and DF omly--RDD is not having 2-D\n",
        "----RDD how data is distributed means suppose ---one data set is like [1,2,3,4,5,6,7,8,910]\n",
        "----one dataset [1,2,3,4] is allocated for one Worker node and rest of the elements are allocated for another worker node\n",
        "----[5,6,7,8,9,10]---DF means contaon rows and columns---low level versions relased RDD--\n",
        "---1.3 above versions relases DF and 1.6 and above dataset\n",
        "---need to install --java 1.8---pyspark---hadoop\n",
        "---set SPARK_HOME=C:\\spark-3.5.2-bin-hadoop3\\spark-3.5.2-bin-hadoop3\n",
        "---set PYSPARK_PYTHON=E:\\anaconda\\python.exe\n",
        "---run pyspark command from command promt then you can see spark env in jupyter notebook and immediately SC object is automatically created\n",
        "---take jupyter notebook URL from command prompt and see in the browser---\n",
        "---in jupyter notebook type the below code and see\n",
        "---\n",
        "\n",
        "----import pyspark\n",
        "----run sc in another jupyter notebook cell and you also run spark means spark session\n",
        "----if you want to work with hive ue spark session other wise we can use SC\n",
        "----if you want to execute SQL related queries SPARK session is necessary.\n",
        "---'''"
      ],
      "metadata": {
        "id": "Wwebiyq7xqU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRf59mNKRBs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlSLzXNlRBqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-VQMJ9mRBnb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}