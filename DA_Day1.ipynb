{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramnagella/Demo/blob/master/DA_Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DA--Day1--29-01-2025"
      ],
      "metadata": {
        "id": "IlLa6OI5qyfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Pre-Processing ---Critical in Data Analysis\n",
        "#Data Pre-Processing ----involves in transforming data into clean,consistant and suitable format for analysis\n",
        "#Essentials of data Pre-Processing ---\n",
        "#1.Data Quality---Rectify the issues like missing data,data consistancies,outliers\n",
        "#1................Check the data quality--preprocessing helpful to rectify the issues like ---missing values,\n",
        "#1..............outliers,inconsistancies and errors.to improive teh data quality and reliability\n",
        "#2.-----Efficiency---Clean and organise the data---saving time and computationalresources---\n",
        "#3..Accuracy---Final o/p is meaningful and reliable\n",
        "#Techniques----\n",
        "#1.Data cleaning --handling the missing values,outliers,inconsistancies,duplicates\n",
        "#2.Data integration---why integration is required in preprocessing ---Data is from different sources after collecting data set\n",
        "# we need to clean and after that should combine from multiple data sources into unified dataset.\n",
        "#Swiggy food ordering application or netflix--based on watching movies we will recomend--\n",
        "#what movied viwed last six mmonths --like category of movies ---gather the data like username ,id,email,history\n",
        "#Based on category will analyse and recomend\n",
        "#Product recomendations in e-commerce like flipcart ---analysed food ordering system also --zomato\n",
        "#Recomendations is ML part\n",
        "#user name .email,jan data based on history,after gathering the data netflix will do analysis .\n",
        "#User account details available inone data source--like web server ,DB server,or local or global or centralised storage\n",
        "#history details is manintaned in some other data source--\n",
        "#paynent details in some other data source--DB\n",
        "#more than 100 movies watched last six months ,,how many are comedy,how many are action and other categories\n",
        "#Collect from the above data sets from different sources and clean the data  set\n",
        "#1..Collect and clean data sets\n",
        "#2..inetgrate all data sets into one unified data set ---like SNO,user,month,category,time, no of movies ,duration of the movie watched\n",
        "#3.payment paid or not,\n",
        "#4.username ,userid,alone\n",
        "#based on the problem or objective prepeare unified data set\n",
        "#Objective is---want yto analyse the particular user watched what categories of movies watched last one year or six months time\n",
        "#unified data set means based on objective or requirement we will create data set\n",
        "# 3---Data transformation ---aggregation,standardisation,Normalisation,Descretization\n",
        "#4---Data reduction ---like dimentionality reduction,feature selection it is under ML part\n",
        "#ML part---Recomendations,predictions,based on history predic the future,\n",
        "#Normalisation ---Removing the duplicate record--redundant data,\n",
        "#Standardisation---Standard format--table format--JSON format,CSV file ,DB format\n",
        "#Dimentionality,Feature Selection is under ML part\n",
        "#"
      ],
      "metadata": {
        "id": "3f4vc-_1q3Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Data Collection---\n",
        "'''\n",
        "Database---files(csv,pdf,excel,json)--snowflae,redshift,sql\n",
        "structured and un-structured data\n",
        "JSON is Semi structured data\n",
        "Structured data --sql,tables RDB file.csv,excel,Avro,rk parquet,\n",
        "Semi structured data ---.JSON,xml\n",
        "unstructured dataa---images,.jpeg,.png,mp3,mp4 and all multimedia files,pdf ,ostly unstructured (text,image)\n",
        "Excel or form will convert into pdf is structured format\n",
        "to read ---csv,tsv,excel,txt,json we will use pandas module in python\n",
        "csv,json,excel have their own modules in python to read the data\n",
        "STPES in data collection----\n",
        "1----Define the objective--goal of the analysis---1.what questions are you trying to answer,2.what problem are you trying to solve\n",
        "improving the efficiency of food delivery process and enhance customer satisfaction\n",
        "2----Determine the data requirements\n",
        "3----Choose the data Source---Select appropriate data sources --like primary and secondary---Ex\n",
        "Objective---Want to analyse the Covid howmany effected and how many recovered---Data sets needs for this purpose--\n",
        "Only for affected releveant data set is available in one data source---any storage --like github(2019,2020,2021,2022)\n",
        "---Recovered relevant data set is available in another data source--DB--repository-2---\n",
        "We need total population details also in the particular state or city --it is mandatory this data set is available in\n",
        "govt data soucres\n",
        "\n",
        "Based on Objectove we can choose the data sources---if you are not geeitng population details it is waste analysis\n",
        "----1...Affected\n",
        "----2---Recovered\n",
        "----3---Population also required to do analysis\n",
        "4--Step is Data Collection methods---like surveys ],interviews,observations,extratc from the existing data\n",
        "5--Step--Data Collection plan---outline the procedure for data collection--what tools/techonlogies used ,for feedback,surveys,\n",
        "6--Step--collect the data--execute the plan and gather required information.\n",
        "7--Ensure data Quality---checks for dupliactes,vaildate survey responses,ensure data correctly formated.\n",
        "---Ex----DOB format(DD-MM-YYYY)\n",
        "9---Store and manage data--organise and store collected data securily into some repositories\n",
        "---Ensureing it is accessible for analysis ---All these are done by BA\n",
        "EX---Swiggy food ordering system---improve Efficiency in food delivery system and enhancing customer satisfaction\n",
        "-----Data requirements---Customer details---(name,address,),order details,transaction details,payment details,feedback information\n",
        "----Primary data source --like swiggy app\n",
        "----secondary data siurce---social media reviews,google ,third party review sites,glass door\n",
        "'''"
      ],
      "metadata": {
        "id": "E-QIUjCl49FN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}