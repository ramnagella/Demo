{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramnagella/Demo/blob/master/DL_NLP_Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AI Definition it is is field .making compurer ir machine intelligent one\n",
        "'''\n",
        "So the machine learning, actually the main important goal is to train the model.\n",
        "\n",
        "With the help of some algorithms will train the model.\n",
        "\n",
        "The model will predict something or else the model will make a decision.\n",
        "So the two things involved in machine learning has a lot of algorithms. Actually the ML has categories into three different types.\n",
        "\n",
        "One is a classification like a supervised learning ,unsupervised and then Reinforcement learning.\n",
        "in ML most of the data set data.It's like a numerical data.\n",
        "\n",
        "Supervised means we are going to train the model.Input as well as the output.\n",
        "The input is otherwise called as a features otherwise called as a independent variable.\n",
        "\n",
        "Output it's called as a label, or otherwise it's called as a dependent variable..\n",
        "\n",
        "So this variable actually depends on the this features.that's why we call it as output, we call it as a dependent variable or a label. OK.\n",
        "\n",
        "Error ---Y-Pred and Y-test are different is called error --interview question\n",
        "To eliminate error --re-train the model or hyper paramaeter tuning means adding/removing some more new features\n",
        "or feature engineering ,accuracy\n",
        "Model means or alg--------interview question---it is prototye ---is a mathematical structure to develope or deploy model\n",
        "\n",
        "integrate two thing --1 data and 2.alg\n",
        "unsupervised learning ---only featueres or i/p no o/p or labels\n",
        "when will use unsupervised---\n",
        "ML is subset of AI ---Main gols of ML is prediction or make decission\n",
        "SL---Classification or regression\n",
        "UnSupervised Learning----clustering or gropuping of records\n",
        "Reinforced Learning ----Sanke game is example rewards or penalties --rienforce learing is used in game developement\n",
        "GenAI is subset of DL ---Without NLPwe cant do anything in GenAI\n",
        "\n",
        "DL----is subset of ML---Why use DL---text data or image data or complex data means will use DL--\n",
        "\n",
        "Automatically extracting the features in DL --in ML it it not possible if want still use ML means we need to\n",
        "convert image intoi mtraix pixel format ---\n",
        "\n",
        "\n",
        "ML work Flow\n",
        "\n",
        "Heart disease and breast cancer problems are under supervised classification\n",
        "and house price prediction ----supervised regression alg\n",
        "\n",
        "Based on history ML algs will predict\n",
        "\n",
        "1. Problem Definition---What is the business requirement\n",
        "2. Data collection ---collect the data from diff sources\n",
        "3.Data Preparation----data pre-processing\n",
        "----cleaning handling missing data or values\n",
        "----duplicated values -\n",
        "----info()\n",
        "----convert data types if required data tye mismatch or not\n",
        "----tansformations ---normalisation and exncoding are transformations\n",
        "----Splitting the features in Data into training and testing\n",
        "----Features(X),Label(Y)\n",
        "4.Feature Engg---means ---identifying the relevant feeatures to train the model\n",
        "-----------------what is important ----\n",
        "----------Feature Selection---\n",
        "----------encoding technique ---\n",
        "----------Feature Scaling ---means giving equal importance to all the features because ML will give more priority or imporatcne for heigher values\n",
        "---------- equal priority for all the features---some times ML algs will biased --\n",
        "----------adding/removing new features like age of the property\n",
        "5.Model Selection---------Model meand it is mathematical structure incorporate ML algs----choosing ML alg or Model-\n",
        "-----------------based label is values or calss we can select the modwl\n",
        "6.Train the model\n",
        "7.Model Evaluation ---performance of the model\n",
        "8.Hyperparameter tuning --in DT max depth one parameter is there is depth level is more means overfitting issue\n",
        "means performance issue is very low\n",
        "chaning the max depth values is hyperparameter tuning---optimisation\n",
        "each and every ML alg hyperparameter tuning is different ---\n",
        "9.Model Testing ---validation for the model for unseen data\n",
        "10.Model Deployment ---web app deployment\n",
        "11.monitoring ---Add/remove new featutres if the preformacne of the model is not good and re-train the model\n",
        "to keep predictions accurate  --monitoring accuracy of the model or error rate\n",
        "12.\n",
        "Explianed Digital Ad car buying example here....\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "M7L3zxCDuNtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day2-----Session--22-01-25\n",
        "'''\n",
        "fit_transform() used for training data and transform() used for testing data.\n",
        "fit() will calculate mean and std deviation for training values and same mean and std deviation will be used for\n",
        "testing values\n",
        "\n",
        "Door Example ..if we want to make 100 doors first will make one door an dtest it for the size of the door opening\n",
        "and based on the first door other doors will made.\n",
        "\n",
        "All other doors will have same width and height\n",
        "\n",
        "Interview question---very important\n",
        "defaukt random_state seed vaules 0 and 42\n",
        "\n",
        "Fit()\n",
        "mean==x1+x2+x3/3\n",
        "std deviation=sqrt(x1-mean)*2+(x2-mean)*2+(x3-mean) whole square\n",
        "Transform()---\n",
        "scalable value =x-mean/std deviation\n",
        "\n",
        "for product based projects need to learn open CV and after NLP GenAI\n",
        "AI engineer learn all, AI engineer use already built model and will develope AI application\n",
        "ML hwo to design the model\n",
        "NLP is vast area with  out NLP we cant learn GenAI\n",
        "\n",
        "Now industry is looking for AI and GenAI Developers next 5 to 10 years\n",
        "\n",
        "how to train the system is AI Engineer ,machine will create and generate anything\n",
        "web3.0 is eveloving all the different techs integrate with web 3.0\n",
        "how to train the machine is going to demand and prompt engineer also demand\n",
        "\n",
        "ML Regression model analysis discussed in this session again ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "A3xpR7zCuNpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day3-----Session--24-01-25--DLsession earlier sessions are on ML\n",
        "'''\n",
        "Encoding techniques ---encoders---\n",
        "\n",
        "---labled data,text data or contextual data converted into numerical data is called encoders--\n",
        "\n",
        "---data like gender or class --why ML undertands only numerical --each and every ML algs mostly(96%) uses maths formulas\n",
        "----statistical formulas---that is the reason will use encoding techniques---\n",
        "----label encider----alphabetical order gives numerical values----onehot encoder---ordinal encoder---mean or target encoder--\n",
        "----if we have distinct categorical data we will use label encoder\n",
        "----ordinal similar to label --education level features phd,pg ,ug ,school\n",
        "----here label encoding is not suitable ,if we want to give rank or order  based on degree will go with ordinal encoder\n",
        "----if we want specified order will use ordinal encoding\n",
        "----mean or target encoding -- when will use this ---if we have high number of categories --phd 10 times ,pg30,ug40,school 100\n",
        "----during this time will calculate mean value\n",
        "----onehot encoding ----\n",
        "----in NLP --one shot ,zero shot ,two shot encoding techniques--\n",
        "----high cardinality dimensions ---cardinality means features---\n",
        "----in a dataset more no of features means high cardinality dimension ---\n",
        "----count encoding,leave one out encoding ,catboost encoding ---\n",
        "----cabin column --titanic data will use count or target encoding\n",
        "\n",
        "----in titani cexample 'Age' is showing as float because of missing values otherwise it is integer\n",
        "---titanic_dataset['Embarked'].fillna(titanic_dataset['Embarked'].mode()[0],inplace=True)\n",
        "---in this line of code mode means heighest occurance of emarked category will get ZERO then next 1 and bext 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "S8RsfhI6uNm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day4-----Session--27-01-25--DLsession\n",
        "'''\n",
        "\n",
        "recently DL community developed breast cancer detection model which is working cery efficiently.\n",
        "\n",
        "Overview of DL---Explains here mind map\n",
        "Elements of DL---\n",
        "DL Libraries-----\n",
        "Application------\n",
        "installation of DL libraries--\n",
        "\n",
        "ANN---Artificial newral networks\n",
        "------in ML machine will not make any decissions ,but in DL machine will make decissions like Humans---\n",
        "------Compared to Ml ,Dl have more naths calculations\n",
        "------In noraml Human Brain to do anya actions it will have normal functions like turn this side or that side\n",
        "------Like similar Brain normal frunctions ANN will have activation function to do the same\n",
        "------i/p----some vent---Brian----o/p\n",
        "------i/p----ANN -----with the help of some naths formula  we can create a fucntion to send the o/p---that\n",
        "------is called activation function---\n",
        "------i/p---hiddenlayer(newrons)----o/p\n",
        "------no of newrons = no of features\n",
        "------if we want we can the features into 10 also in DL\n",
        "------we can reduce again in next hidden layer into 7 newrons\n",
        "------each and every layers have multiple newrons\n",
        "------fully coinnected layer---convulation layer---pooling layer\n",
        "------weight means it is like a edge between adjacent nodes or distance\n",
        "------events or actions are stored in the newrons\n",
        "------abraham lincon image with pixel gray scale into matrix--\n",
        "------convolution layer ---take small part by part of picture ---\n",
        "------it will extract the features----\n",
        "------So each and everything it's having some colors OK.\n",
        "\n",
        "So this colors actually converting into the for example this one is a a light ash color, so the corresponding RGB value will be replacing OK.\n",
        "\n",
        "So this one actually will be going to changing the RGB values so likewise.\n",
        "\n",
        "Dark black color or light or black color.\n",
        "\n",
        "So likewise the entire images will be going to convert into the RGB values.\n",
        "\n",
        "See the mindmap in introduction to DL in Github for more details on theory on DL---\n",
        "\n",
        "if we are processing images we will these CNN layers like convolution and pooling layers\n",
        "if we have covid 19 data in numerical format we can use fully connected layers\n",
        "images means convolution layers\n",
        "if we have covid 19 in images like x-ray we can use convolution layers\n",
        "student age degree marks admission eligibilty for IIT or NIT we wil fuly connexted layers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "''''\n"
      ],
      "metadata": {
        "id": "337AbvLwuNkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day5-----Session--29-01-25--DLsession\n",
        "\n",
        "'''\n",
        "Agenda\n",
        "\n",
        "1.Weights and Bias\n",
        "2.types of NN\n",
        "3.Applications\n",
        "4.Installations  of DL and NLP libraries\n",
        "\n",
        "\n",
        "for example to identify a image is car,or bus or truck,or tractor means\n",
        "\n",
        "first will use no of hidden layers to check it ,each hidden layer can have many no of newrons and finally four newrons to o/p\n",
        "weights gives importance to input features when NN procressing the  data\n",
        "weights important features\n",
        "weight is edge connecting between adjcent newron.weights are adjusted during training for better results\n",
        "gibve the imporatnce of the important features\n",
        "\n",
        "Bias ---x1 feature sq feet no of bed rooms another feature third feature not taking into consideration\n",
        "because to calculate the price it is not required\n",
        "X1=weight=.5 X2=.3\n",
        "Feature scaling in DL is very important and differnet from ML==Z=x1w1+x2W2\n",
        "\n",
        "less weight value means low influence and high weight means high influence\n",
        "DL will calculate automatically the weight values\n",
        "weight most of the times will in between 0 to 1 like probability\n",
        "if x1 and x2 values are missing in the dataset at that time Bias will help\n",
        "\n",
        "if feature values are missing Bias will help and Bias is fixed value .DL will fix the Bias Vakue\n",
        "\n",
        "Activations ---Step fucntion 0 to 1 values either +ve or -Ve\n",
        "Sigmoid function ---+ve infinity to -ve infinity ---these both binary classification functions\n",
        "\n",
        "softmax ---multiclass o/p\n",
        "\n",
        "changing the activations functions will imporve the performance .it is also called hyper parameter tuning\n",
        "\n",
        "EeLu fucntion is used in between hidden layers because in image matrices\n",
        "the image RGB values some values +ve and some -ve .ReLu function make -Ve values into ZERO\n",
        "and +ve as it is means kepp the same value  .\n",
        "Step ,sigmoid and softmax activation fucntions are used in o/p layers.\n",
        "\n",
        "Hyper parameter tuning in DL ---improve the model performance ---\n",
        "dropout---some features not required we can droo which will improve the performance\n",
        "learning rate ----means how the model update weights during the training --its determins teh step size\n",
        "-----------------tinysteps or large steps ---\n",
        "---The learning rate determines the step size during optimization\n",
        "1 lacs record ---step size 2 means it will take more time\n",
        "1000 recirds ----2 records each time it is tiny steps  it will take more time it will take 500 steps it is learning rate\n",
        "1000 records---100 records----will give result\n",
        "no of epochs means training count ----\n",
        "batch size-----10000 records ---batch one 100 records ---batch two size 100\n",
        "batch size dictates how many training samples are processed before the model's weights are updated\n",
        "newrons ----no of newrons in hidden layers\n",
        "\n",
        "\n",
        "\n",
        "x\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "bc9ReokiuNXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day6------Session--31-01-25--DLsession\n",
        "\n",
        "'''\n",
        "RNN---Recurrent neural networks---means if the input is not clear it will serach in the previous searches and\n",
        "get the senctence like weather in if tyoe and leave it will previous searches results weather in banglore\n",
        "in google search engine --whatsapp and any other search enignes will use RNN\n",
        "\n",
        "\n",
        "Installations--\n",
        "in visual studio\n",
        "\n",
        "install Python\n",
        "\n",
        "extensions ---python\n",
        "--------------pylance\n",
        "--------------pip installer\n",
        "--------------jupyter notebook cell tags ,notebook,vscode\n",
        "--------------DL important libraries\n",
        "--------------tensorflow,keras,pandas,numpy,scipy,pytorch\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip installpandas\n",
        "!pip install numpy\n",
        "!pip install pytorch\n",
        "!pip install scipy\n",
        "!pip install h5py which is required to store weights of newrons\n",
        "!pip install scikit-learn -- for ML algs if required in DL also\n",
        "\n",
        "without model we cant deploy any applications in Cloud like AWS azure\n",
        "there are pre -trained models are there in huggungface.it is flatform\n",
        "or build models from scratch and use it\n",
        "\n",
        "how to access models from huggingface\n",
        "we need tarnsforms to download the models from huggingaface--\n",
        "more than 1 GB model we cant upload into the HF\n",
        "---Creating ANN design program is there Simple Neural network design----\n",
        "Dense layer means fully conncted layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "mTJKmDVluNUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day6------Session--03-Feb-25--DLsession\n",
        "\n",
        "'''\n",
        "Simple neural network design\n",
        "Neural network---1.Percptron ----only one o/p will choose this one\n",
        "-----------------2.Multiple perceptron--two o/p means choose this one\n",
        "-----------------3.FNN Feed forward neural network--teo o/p means choose this one also\n",
        "-----------------4.Convolutional neural network---image means choose this one\n",
        "-----------------5.Recurrent neural network-------text means choose this one chatbot,text classification ,text generation\n",
        "-----------------along with RNN will use additional algs\n",
        "-----------------6.GAN--Genrative ad neural network\n",
        "Discussed---how to calculate no of neurons in each layer\n",
        "------------no of hidden layers\n",
        "------------Optimisers\n",
        "------------Withoput hidden layer can we create NN ? Question if possible how if no why\n",
        "------------\n",
        "\n",
        "# Adding Layers:\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))#hidden layer +input in the first\n",
        "in this hidden layer 12 neurons are there and input layer have 8 columns or features\n",
        "model.add(Dense(8, activation='relu'))---hidden layer 2\n",
        "model.add(Dense(1, activation='sigmoid'))---output activation function\n",
        "\n",
        "for structured data\n",
        "<10,000 records means 1 or 2 hidden layers aere enough\n",
        "shallow networks 1 or 2 hidden layers\n",
        "medium dataset 10k to 100k records go with 2 or 3 hidden layers\n",
        "above 100K records go with 3 to 4 hidden layers--for large dataset\n",
        "128--64--32---16 in each hidden layers for large dataset --speech or text data or image data\n",
        "\n",
        "no of neurons for structured data\n",
        "no rule is there to gove no of neurons in hidden layers\n",
        "Twice o/p neurons +input neuron---2*2=4+8=12 neurins in first hidden layer\n",
        "\n",
        "hidden layer neuron just reduce the neurons approximately\n",
        "if you take incorrect neurons in hidden layers it will not form neural network\n",
        "during testing and training model  wecan see the impact\n",
        "if we have image data everything is pixel ---RGB value image and gray scale image\n",
        "\n",
        "image dimension small images 28*28*3 means height width and colour 3 means colour\n",
        " one means gray scale\n",
        " 28*28*1----\n",
        "\n",
        "medium image 128*128*3 or 1\n",
        "large image 1024*1024*3 or 512*512*3 or 1\n",
        " for tabular data 2 hidden layers are enough\n",
        "for images 5 hidden are required CNN is used for images\n",
        "small images 28*28 3 to 5\n",
        "medium images  5 to 10 layers\n",
        "high images ---10+ layers\n",
        "\n",
        "how many neurons will take for input ---\n",
        "10*10 image if it is color 10*10*3=300 neurons are reuired in imput layer\n",
        "for regresion you need only one neuron in the o/p--becasue o/p is one value\n",
        "Each and every hidden layer have one convolution and one pooling layer\n",
        "\n",
        "optimisers---purpose ---minimise/reduce the\n",
        "\n",
        "after NN design will use optimiser\n",
        "\n",
        "optimiser will update the wieghts during training\n",
        "adam ,--suits for all most all DL --best general\n",
        "schochastic,--good for Deep n/w\n",
        "RMSProop,---RNN and sequential--suits fior regression problem\n",
        "SGD---suits for image--simple model but slow\n",
        "AdaGrade,--best for sparse data---NLP and embedded techniques\n",
        "adaMax,---NLP and embedded techniques\n",
        "Adadelta.Rienforcement learning\n",
        "Nadam--adam + momentum\n",
        "GAN---GAN stands for generative adversarial network. It's a type of machine learning model called a neural network, specially designed to imitate the structure and function of a human brain. For this reason,\n",
        " neural networks in machine learning are sometimes referred to as artificial neural networks (ANNs)application---\n",
        "\n",
        "\n",
        "\n",
        " -----------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "_TKeeBA5uNRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day6------Session--07-Feb-25--DLsession\n",
        "\n",
        "'''\n",
        "Covid-19 Detection using X-Rays\n",
        "\n",
        "We ust to know the simple procedure how to design A neural networks and\n",
        "how to build a model and how to save the model and\n",
        "how to load the pre trained model and\n",
        "how to predict the input, unknown data based on that,\n",
        "how to predict it, this is the way we can just know\n",
        "how can we generate the image?\n",
        "\n",
        "DL is completed ---Only Covid problems explained CNN and simple neural network design.\n",
        "\n",
        "Now started NLP training\n",
        "Tokenization---\n",
        "stemming ---removing suffixes to bring it to base form or root form .\n",
        "former,format,forming ---stemming will form but it is wrong\n",
        "lematization---it retutns a correct word\n",
        "better,super ==lema returns good\n",
        "\n",
        "Sentiment Analysis---example -----\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Y7gFCBFquNOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day6------Session--10-Feb-25--NLP Session\n",
        "\n",
        "'''\n",
        "tokenisation 1.sentecne tokenisation 2. word tokenisation\n",
        "\n",
        "nltk.download('punkt_tab')# breaking down large text into sentecne and words withour manually labelled training data\n",
        "\n",
        "sent=sent_tokenize(text)\n",
        "\n",
        "# GEt English stopwords\n",
        "these are common words it will reduce the text size.\n",
        "improve model accuracy\n",
        "enhance the search results\n",
        "\n",
        "stop_words=set(stopwords.words('english'))# is reuiqred to improve the efficiency in text classification,\n",
        "search engines,sentment analysis\n",
        "\n",
        "When to add /remove stop words\n",
        "--text analysis ,search enigne --remove stop words\n",
        "--quwestions and answers--sentiment analysis ---text generation----tense and grammatical rule are important --\n",
        "---dont remove stopwotds\n",
        "--stopwords.fileids()--will give list of different languages stop words\n",
        "--- len(stopwords.fileids())\n",
        "\n",
        "former,forming,format here form as base or root word and happines---will happi it is meaning less\n",
        "porterstemmer developed martin stemmer developed in 1980--\n",
        "rule based reduction technique---\n",
        "removes suffixe like ing,ed,es\n",
        "remove plural cats cat\n",
        "handle tense normalisation\n",
        "Limitations of Porter Stemmar---like better\n",
        "fair\n",
        "easily fairly will get easi,fairli  may produce non-existing words\n",
        "oerstemming issue ---like universal--will get --univers--it is not right\n",
        "to avoid these issues will use Lancasterstemmar --it is more aggressive\n",
        "that is the reason will go with Lematisation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "0hhTY_Q6uNLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DL_NLP_Day6------Session--12-Feb-25--NLP Session\n",
        "\n",
        "'''\n",
        "Documenst means each and every sentence or text in the record is called document\n",
        "\n",
        "feature extraction---important features---\n",
        "In this feature extraction, we can use one technique that is called as a vectorization\n",
        "We can convert in text into the numerical like a matrix format that is called as a vectorization.\n",
        "extracting the uniquee words from the text\n",
        "So vectorization means any kind of text format it will convert into the numerical like a matrix format.\n",
        "\n",
        "There are three different vectorization actually used.\n",
        "1 is a count vectorization and another one is hashing vectorization,\n",
        " hash vectorization and another one it's a very very important TF IDF vectorization.\n",
        "\n",
        "Any cetorizer purposes, we can convert from text to format into the numerical format.\n",
        "\n",
        "The numerical format actually we will get the matrix format that is called as a sparse matrix,\n",
        "that is called as a sparse matrix.\n",
        "\n",
        "CountVectorizer Example Explained\n",
        "steps for the below text data\n",
        "documents = [\"I love Python programming.\",\n",
        "\"Python is great for AI and ML.\",\n",
        "\"AI and ML are the future.\"]\n",
        "Let's break down the CountVectorizer process step by step for the given documents :\n",
        "python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Step 1: Define the documents\n",
        "documents = [\"I love Python programming.\",\n",
        "              \"Python is great for AI and ML.\",\n",
        "              \"AI and ML are the future.\"]\n",
        "# Step 2: Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# Step 3: Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "# Step 4: Extract the vocabulary\n",
        "vocab = vectorizer.vocabulary_\n",
        "# Step 5: Convert the sparse matrix to an array\n",
        "matrix = X.toarray()\n",
        "# Step 6: Extract feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "# Display results\n",
        "print(\"Step 4: Vocabulary (Word to Index Mapping):\", vocab)\n",
        "print(\"\\nStep 5: Sparse Matrix Representation:\\n\", matrix)\n",
        "print(\"\\nStep 6: Feature Names:\", feature_names)\n",
        "1/10\n",
        "Step-by-Step Process for Given Text Data\n",
        "Step 1: Input Text Data\n",
        "Step 2: Tokenization\n",
        "The text is split into individual words.\n",
        "Step 3: Lowercasing\n",
        "Step 4: Vocabulary Creation\n",
        "A unique index is assigned to each word.\n",
        "Step 5: Sparse Matrix Representation\n",
        "Each row corresponds to a document, and each column corresponds to a word.\n",
        "ai and are for future great is love ml programming py\n",
        "Doc\n",
        "1\n",
        "0 0 0 0 0 0 0 1 0 1 1\n",
        "arduino\n",
        "[\"I love Python programming.\",\n",
        "\"Python is great for AI and ML.\",\n",
        "\"AI and ML are the future.\"]\n",
        "css\n",
        "['I', 'love', 'Python', 'programming']\n",
        "['Python', 'is', 'great', 'for', 'AI', 'and', 'ML']\n",
        "['AI', 'and', 'ML', 'are', 'the', 'future']\n",
        "css\n",
        "['i', 'love', 'python', 'programming']\n",
        "['python', 'is', 'great', 'for', 'ai', 'and', 'ml']\n",
        "['ai', 'and', 'ml', 'are', 'the', 'future']\n",
        "bash\n",
        "{'love': 5, 'python': 7, 'programming': 6, 'is': 3, 'great': 2, 'for': 1,\n",
        "'ai': 0, 'and': 4, 'ml': 8, 'are': 9, 'the': 10, 'future': 11}\n",
        "2/10\n",
        "ai and are for future great is love ml programming py\n",
        "Doc\n",
        "2\n",
        "1 1 0 1 0 1 1 0 1 0 1\n",
        "Doc\n",
        "3\n",
        "1 1 1 0 1 0 0 0 1 0 0\n",
        "Step 6: Feature Names\n",
        "Summary\n",
        "Tokenization → Splits sentences into words.\n",
        "Lowercasing → Converts to lowercase.\n",
        "Vocabulary Building → Assigns an index to each unique word.\n",
        "Sparse Matrix → Represents word occurrences in each document.\n",
        "Would you like to see a version with stopwords removed or n-grams included? ��\n",
        "css\n",
        "['ai', 'and', 'are', 'for', 'future', 'great', 'is', 'love', 'ml', 'programming',\n",
        "'python', 'the']\n",
        "Example: TF-IDF Vectorizer with Step-by-Step Explanation\n",
        "TF-IDF (Term Frequency - Inverse Document Frequency) helps assign importance to words\n",
        "based on their frequency in a document and across all documents. Unlike CountVectorizer ,\n",
        "which counts raw word occurrences, TfidfVectorizer assigns weights, reducing the impact\n",
        "of common words.\n",
        "Python Code\n",
        "python\n",
        "3/10\n",
        "Step-by-Step Explanation\n",
        "Let's break down the steps for the given documents .\n",
        "Step 1: Input Text Data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Step 1: Define the documents\n",
        "documents = [\"I love Python programming.\",\n",
        "\"Python is great for AI and ML.\",\n",
        "\"AI and ML are the future.\"]\n",
        "# Step 2: Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Step 3: Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "# Step 4: Extract the vocabulary\n",
        "vocab = vectorizer.vocabulary_\n",
        "# Step 5: Convert the sparse matrix to an array\n",
        "matrix = X.toarray()\n",
        "# Step 6: Extract feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "# Display results\n",
        "print(\"Step 4: Vocabulary (Word to Index Mapping):\", vocab)\n",
        "print(\"\\nStep 5: TF-IDF Matrix Representation:\\n\", matrix)\n",
        "print(\"\\nStep 6: Feature Names:\", feature_names)\n",
        "arduino\n",
        "[\"I love Python programming.\",\n",
        "\"Python is great for AI and ML.\",\n",
        "\"AI and ML are the future.\"]\n",
        "4/10\n",
        "Step 2: Tokenization\n",
        "The text is split into individual words.\n",
        "Step 3: Lowercasing\n",
        "Step 4: Vocabulary Creation\n",
        "A unique index is assigned to each word.\n",
        "Step 5: TF-IDF Calculation\n",
        "Each row corresponds to a document, and each column corresponds to a word.\n",
        "The TF-IDF formula is:\n",
        "Where:\n",
        "TF (Term Frequency): How often a word appears in a document.\n",
        "IDF (Inverse Document Frequency): Measures how unique a word is across documents.\n",
        "ai and are for future great is love ml pro\n",
        "Doc\n",
        "1\n",
        "0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.707 0.000 0.70\n",
        "css\n",
        "['I', 'love', 'Python', 'programming']\n",
        "['Python', 'is', 'great', 'for', 'AI', 'and', 'ML']\n",
        "['AI', 'and', 'ML', 'are', 'the', 'future']\n",
        "css\n",
        "['i', 'love', 'python', 'programming']\n",
        "['python', 'is', 'great', 'for', 'ai', 'and', 'ml']\n",
        "['ai', 'and', 'ml', 'are', 'the', 'future']\n",
        "bash\n",
        "{'love': 5, 'python': 7, 'programming': 6, 'is': 3, 'great': 2, 'for': 1,\n",
        "'ai': 0, 'and': 4, 'ml': 8, 'are': 9, 'the': 10, 'future': 11}\n",
        "TF-IDF = TF × IDF\n",
        "5/10\n",
        "ai and are for future great is love ml pro\n",
        "Doc\n",
        "2\n",
        "0.333 0.333 0.000 0.333 0.000 0.333 0.333 0.000 0.333 0.00\n",
        "Doc\n",
        "3\n",
        "0.408 0.408 0.471 0.000 0.471 0.000 0.000 0.000 0.408 0.00\n",
        "Step 6: Feature Names\n",
        "Comparison with CountVectorizer\n",
        "Feature CountVectorizer TfidfVectorizer\n",
        "Output Count of words Weighted score\n",
        "Impact High frequency words dominate Penalizes common words\n",
        "Use case Simple text representation Information retrieval, NLP tasks\n",
        "Would you like an example with stopwords removed or bi-grams added? ��\n",
        "css\n",
        "['ai', 'and', 'are', 'for', 'future', 'great', 'is', 'love', 'ml', 'programming',\n",
        "'python', 'the']\n",
        "Let's calculate the TF-IDF score for the word \"Python\" in Document 1 manually step by step.\n",
        "Step 1: Understanding the TF and IDF Formulas\n",
        "1. Term Frequency (TF) Formula\n",
        "2. Inverse Document Frequency (IDF) Formula\n",
        "TF =\n",
        "Total number of terms in the document\n",
        "Number of times the term appears in the document\n",
        "( )\n",
        "6/10\n",
        "3. TF-IDF Formula\n",
        "Step 2: Compute TF for \"Python\" in Doc 1\n",
        "Document 1: \"I love Python programming.\"\n",
        "Tokenized words: [\"I\", \"love\", \"Python\", \"programming\"]\n",
        "Total words: 4\n",
        "Occurrences of \"Python\": 1\n",
        "Step 3: Compute IDF for \"Python\"\n",
        "Total documents: 3\n",
        "Documents containing \"Python\": 2 (Doc 1 & Doc 2)\n",
        "Approximating logarithm:\n",
        "Step 4: Compute TF-IDF for \"Python\"\n",
        "IDF = log ( +\n",
        "Number of documents containing the term\n",
        "Total number of documents ) 1\n",
        "TF − IDF = TF × IDF\n",
        "TF(\"Python\") = =\n",
        "4\n",
        "1\n",
        "0.25\n",
        "IDF(\"Python\") = log ( +\n",
        "2\n",
        "3) 1\n",
        "log(1.5) ≈ 0.1761\n",
        "IDF(\"Python\") = 0.1761 + 1 = 1.1761\n",
        "TF − IDF = 0.25 × 1.1761 = 0.294\n",
        "7/10\n",
        "Step 5: Compare with Scikit-learn\n",
        "Manual TF-IDF for \"Python\" in Doc 1: 0.294\n",
        "Scikit-learn TF-IDF for \"Python\" in Doc 1: 0.4736\n",
        "�� Why is it different?\n",
        "Scikit-learn normalizes the values using L2 normalization (default).\n",
        "This means the values are scaled to ensure vector magnitude = 1, improving text\n",
        "similarity calculations.\n",
        "Would you like me to compute TF-IDF without normalization for an exact match? ��\n",
        "You're right! Let's verify why Scikit-learn's TF-IDF for \"Python\" in Doc 1 is 0.4736 and where\n",
        "our manual calculation differs.\n",
        "Step 1: Understanding TF-IDF in Scikit-learn\n",
        "Scikit-learn applies L2 normalization by default, which means it scales the TF-IDF vectors so\n",
        "that their Euclidean norm (L2 norm) equals 1.\n",
        "Step 2: Manual Calculation of TF-IDF for \"Python\" in Doc 1\n",
        "1. Compute Term Frequency (TF)\n",
        "Doc 1: \"I love Python programming.\"\n",
        "Tokenized words: [\"I\", \"love\", \"Python\", \"programming\"]\n",
        "Total words: 4\n",
        "Occurrences of \"Python\": 1\n",
        "2. Compute Inverse Document Frequency (IDF)\n",
        "TF(\"Python\") = =\n",
        "4\n",
        "1\n",
        "0.25\n",
        "8/10\n",
        "Total documents: 3\n",
        "Documents containing \"Python\": 2 (Doc 1 & Doc 2)\n",
        "Approximating logarithm:\n",
        "3. Compute Raw TF-IDF (Before Normalization)\n",
        "This raw TF-IDF value (0.351) differs from 0.4736. The difference comes from L2\n",
        "normalization.\n",
        "Step 3: Apply L2 Normalization\n",
        "L2 normalization scales the entire document vector so that:\n",
        "We need to calculate the L2 norm of Doc 1's vector and divide each TF-IDF score by it.\n",
        "1. Retrieve all TF-IDF values in Doc 1 before normalization\n",
        "Let's assume Scikit-learn computed:\n",
        "2. Compute L2 Norm of the Vector\n",
        "3. Normalize the TF-IDF Score for \"Python\"\n",
        "IDF(\"Python\") = log ( +\n",
        "2\n",
        "3) 1\n",
        "log(1.5) ≈ 0.405\n",
        "IDF(\"Python\") = 0.405 + 1 = 1.405\n",
        "TF − IDF(\"Python\") = 0.25 × 1.405 = 0.351\n",
        "∑(TF-IDF values) =\n",
        "2 1\n",
        "csharp\n",
        "[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.6228, 0.000, 0.6228, 0.4736,\n",
        "0.000]\n",
        "(0.6228)\n",
        "2 + (0.6228)\n",
        "2 + (0.4736)\n",
        "2 = 0.3879 + 0.3879 + 0.2243 = 1.0001 ≈ 1.0\n",
        "9/10\n",
        "�� This confirms that Scikit-learn's 0.4736 is correct due to L2 normalization.\n",
        "Final Conclusion\n",
        "1. Raw TF-IDF (without normalization): 0.351\n",
        "2. Scikit-learn TF-IDF (with L2 normalization): 0.4736\n",
        "Would you like me to calculate the TF-IDF of other words in the document for comparison?\n",
        "��\n",
        "=\n",
        "1\n",
        "0.351\n",
        "0.4736\n",
        "10/10\n",
        "\n",
        "##############################################################\n",
        "Count Vectorization Steps\n",
        "\n",
        "step-by-step explanation of how vocabulary is created in Count Vectorization\n",
        "Step-by-Step: Vocabulary Creation in Count Vectorization\n",
        "1. Input Collection:\n",
        "a. You start with a collection of documents or sentences, called a corpus.\n",
        "b. Example:\n",
        "i. Document 1: \"I love machine learning\"\n",
        "ii. Document 2: \"Machine learning is fun\"\n",
        "2. Text Normalization (optional but typical):\n",
        "a. Convert all text to lowercase to ensure consistency.\n",
        "b. Result:\n",
        "i. \"i love machine learning\"\n",
        "ii. \"machine learning is fun\"\n",
        "3. Tokenization:\n",
        "a. Break each document into individual words (tokens).\n",
        "b. Document 1: [\"i\", \"love\", \"machine\", \"learning\"]\n",
        "c. Document 2: [\"machine\", \"learning\", \"is\", \"fun\"]\n",
        "4. Unique Word Identification:\n",
        "a. Combine tokens from all documents.\n",
        "b. Identify all unique words (this becomes your vocabulary).\n",
        "c. From the example:\n",
        "i. All tokens combined: [\"i\", \"love\", \"machine\", \"learning\", \"machine\",\n",
        "\"learning\", \"is\", \"fun\"]\n",
        "ii. Unique words (vocabulary): [\"i\", \"love\", \"machine\", \"learning\", \"is\",\n",
        "\"fun\"]\n",
        "5. Assigning Indexes to Words:\n",
        "a. Each unique word in the vocabulary is assigned a fixed index.\n",
        "b. Example:\n",
        "i. \"i\" → 0\n",
        "ii. \"love\" → 1\n",
        "iii. \"machine\" → 2\n",
        "iv. \"learning\" → 3\n",
        "v. \"is\" →\n",
        "This dictionary of word → index is the vocabulary.\n",
        "6. Result:\n",
        "a. The vocabulary is now a mapping of each unique word to an index.\n",
        "b. This vocabulary is later used to transform any input text into a count vector\n",
        "(word frequency vector).\n",
        "\n",
        "\n",
        "###########################################################\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CFpLhWKFuNI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJ3egVG5uNGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyV8uk_ouNDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-jdxSrzuNBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "027AR1JauM-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHmlNUFUuM2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZjO0FwUuMzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xI4oRp7ouMwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}