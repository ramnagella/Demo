{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramnagella/Demo/blob/master/ML_Day4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25-10-2024 Session ----Logistic Regression you can see in gitgub\n",
        "#80% data will be selected for training and 20% for testing\n",
        "#total data is devided into training and test data based on 80% of the data with the help of ML Algs ML model is developed and\n",
        "#then will give test data ,ML model will predict the o/p or label that is Y_Predict values\n",
        "#and Will validate Y_predict with y_test values after the validation will get accuracy"
      ],
      "metadata": {
        "id": "CjlyN2y5m4QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' ML work Flow---1. Problem Statements 2.Data Collection 3.Data Cleaning or Preparation\n",
        "4.Choosing the algorithm 5.Train or build the data 6. Test 7.Evaluation if accuracy is less then choose another algorithm\n",
        "8. After biulding the model ,Deploy the model\n",
        "\n",
        "Here in our example we can add another column/feature like age group for the people aging between 20 t- 30 and 30 to 40\n",
        "Data preparation involves ---1 cleaning 2. transformatio  process and 3 EDA\n",
        "Best example for logistic regresion is email spam or not\n",
        "\n",
        "1. binomial logistic regression(true or false or 0 or 1) 2.multinomial logistic regression  more than two (fruits types animal types\n",
        "and ordinal means three or more types of dependent variables low medium high like that\n",
        "Same data we can use it another algs\n",
        "dataset.duplicated()\n",
        "dataset.drop_duplicates()\n",
        "dataset['Age'].duplicated\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q1bDyZTum4MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28-10-2024--After splitting the dat into train and test rest of the code coninued in the next session on  28-octt\n",
        "'''\n",
        "Feature scaling machine sould give equal priority for all the features because ML algs will give priority for\n",
        "bydeafult for heigher values like age 20 salary 80000 ML algs will give priority for 80000 salary forst then age\n",
        "to avoid this will do feature scaling which will avoid bias\n",
        "Will use standardisation feature scaling and normalisation feature scaling\n",
        "by using all these feature scling techiniques machine will give equal priority for all the features\n",
        "\n",
        "Normalisation actually method is useful when working with the distance based algorithms or neural networks where values need to be in a specific range\n",
        "some of the distance calculations you can using the this one support vector machine\n",
        "'''"
      ],
      "metadata": {
        "id": "sYjbkI54m4Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Regression is under the regression algorithm concept --\n",
        "#Here we are taking house price prediction\n",
        "'''\n",
        "In this example only two features are there that is the reason we no need to go for feature scaling\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "ooXulbxhm4BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "04-11-2024------Session ---Linear Regression ---\n",
        "\n",
        "if we are using classificatiuon algs better to use accuracy score for validation\n",
        "\n",
        "supervised approach  Classification algs like logistic regression will Accuracy for measuring accuracy\n",
        "in supervised approach Regression algs like linear regression and other algs will use R square ,MAE(Mean absolute error),MSE(Mean square error)\n",
        "\n",
        "If your algs are not giving right results for training and test we can generate data for sample testing purpose using\n",
        "Faker module in Python suppose you have only records of data like 10 rowss and 4 columns we can generate 1000 rows and 15 columns of data\n",
        "using Faker module/library based on our 10 rows and 4 columns data\n",
        "\n",
        "We are using linear equation for linear regression there are lot of equations to use but we are using linear\n",
        "in linear regression draw best fit line for all the data points (cover all the daat points ) then we can find\n",
        "slope and intercept\n",
        "\n",
        "if one feature is there we can use Y=mx +c\n",
        "if mutiple features arre there we can use Y=m1x1+m2x2+c\n",
        "There are different ways like single i/p and single o/p---Single linear regressio\n",
        "multiple i/p and single o/p---multiple linear regression\n",
        "multiple i/p and multiple o/p---is not there in linear regression\n",
        "Slope =DY/DX\n",
        "\n",
        "House price data is already cleaned one we took and no missing values\n",
        "we are visualsing the data for Linear regression\n",
        "\n",
        "Plotting teh values in X-Y plane\n",
        "\n",
        "Segregation code\n",
        "X=dataset.iloc[:,:-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "Here no need to split the data into training and test if qwe want we can do it but not required\n",
        ".fit() ---method by passing  i/p and o/p fit or biuld a model\n",
        "\n",
        "While validating if we are giving x value if there is lot of variation is there means need more data tp train\n",
        "Same example will try with other algorithm since there is lot variance in the prediction values of house prices\n",
        "with Linear regression alg\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model=DecisionTreeRegressor()\n",
        "model.fit(X_train,y_train)#or model.fit(X,y)\n",
        "Then use the prediction code in the program\n",
        "Here we are getting approx correct value\n",
        "y_predict=model.predict(X_test)\n",
        "y_predict=model.predict(X_test)\n",
        "So DecisionTreeRegressor alg is good for this problem\n",
        "DecisionTreeClassifier alg also is there but it is used classification problems not for this example .\n",
        "Since the data is continuos so we are choosing regression algs not classificaton algs\n",
        "\n",
        "In  2nd case some of the values are missing ofr hours columan it would be better to fill with avg value instead of nulll or zero value\n",
        "\n",
        "dataset['hours'].isna().sum()  ---to check the null values in hours column alone\n",
        "Predict exam marsks for any values\n",
        "data=[[8.67,19,1]]\n",
        "model.predict(data)\n",
        "run the above code and verify the result with actuals .0.1 difference it is good alg for this problem\n",
        "\n",
        "For regression to check the performace will use metrics mostly like measn square error,mean absolute error,r2.score\n",
        "\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GRfqq5jam3-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#06-11-2024-----Session ---Decission Tree classifier alg\n",
        "'''\n",
        "Decission tree supports bith classification and regression\n",
        "How to draw decission tree root node and decission nodes for ex age,weight,height and health status\n",
        "status will be the last node in the tree called leaf node\n",
        "here we can take first age is root nore if age <18 yes next D node then if weight <50 if yes another D node if height <100\n",
        "\n",
        "other example load id --01 means home loan,2--car loan,3 site loan loan amount 10000,20000,3000 loan status good or bad\n",
        "first two columns are i/p features status is o/p or label\n",
        "\n",
        "How to draw D Tree--1.info gain 2.entropy.3,Gini index or Gini impurity are three ways we can select root node\n",
        "in a basket we stored only apple means it is pure\n",
        "if many fruits are stored it is impured\n",
        "popcorn,soda,icecream people loves in cinema\n",
        "\n",
        "loves popcorn yes or no in next node two people loves soda ad 0 people loves Ice if it comes with combination of\n",
        "yes and no it is impure if either YES or No means it is pure if it is impure means go for another level of decision tree\n",
        "impurity means again split the node into another branch\n",
        "how to  find root node is interview question\n",
        "See the video of Decision and classification trees clearly explained\n",
        "\n",
        "Gini impurity of the leaf = 1- (probability of YES)*2 -(probability of NO)*2\n",
        "if the both leaves do not represent same no of people then total Gini impurity if wighted avg of the\n",
        "leaf impurities\n",
        "\n",
        "Then we will multiply the weight by its impurity ---Weight =total no of people in leaf/total no of people in both the leaves\n",
        "\n",
        "Here we need to calculate three Gini impurity values for Age,Popcorn and soda among three which one is minimal impurity\n",
        "that node we can take as root node ,but alg will take automatically take the root node we no need to worry\n",
        "if there are many no of features are there means it is will calculate manually and select root node\n",
        "\n",
        "The age column is numerical values first sort the age column and calculate avg of age adjacent people like 7+12=19/2=9.5\n",
        "then 12+18=30/2=15 like that need to calulate the avg of adjacent values\n",
        "then if age <9.5 cool as ice YES or NO calculate impurity\n",
        "Gini impurity =1- (probability of YES)*2 -(probability of NO)*2\n",
        "if the both leaves do not represent same no of people then total Gini impurity if wighted avg of the\n",
        "leaf impurities\n",
        "\n",
        "Then we will multiply the weight by its impurity ---Weight =total no of people in leaf/total no of people in both the leaves\n",
        "\n",
        "After root node we need to select the next node among the two  nodes which node having more impurity we will take that node as decission node\n",
        "Calculating the max_depth=\n",
        "\n",
        "Working principle of decission tree algs are very important ----\n",
        "\n",
        "First"
      ],
      "metadata": {
        "id": "mrKrD-sXm37b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "08-11-2024---Session -------Decission Tree Classifier/Regressor --Decission tree supports both classification and regression\n",
        "'''\n",
        "To avoid overfitting max_depth of decission tree should be increased.---interview question --it is also hyper parameter tuning\n",
        "Tuning ----\n",
        "\n",
        "This is issue in Decission Tree ---Overfitting means when we are giving inseen data we are getting wring answers ----if the accuracy is same for\n",
        "3,4 5th and 6th depth levels sto there no need to go or increase depth level this issue called overfitting --because we are already got\n",
        "accuracy--Max split is also used  to hyper tune\n",
        "\n",
        "OUTLier---this is issue in Linear regression issue----Means some of the data points are missing the Linear regression line the best fit line not covers some of\n",
        "the data points--\n",
        "To over come this problem --detect the outlier --with statistiacl method swe can find out this issue\n",
        "cook effect,IQR ,z-score detect and remove outliers ---\n",
        "with threshold value we can remove outlier after that we can traing the mocel again and the error rate will come down\n",
        "\n",
        "Decission Tree classifier we aere using iris dataset ....\n",
        "\n",
        "Label Encoder will take by default in alphabetical order\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Lxe8h4tHzjUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11-08-2024---Session ---Encoder techniques\n",
        "'''\n",
        "\n",
        "Encoder ---Categorical variable values converted into numerical values\n",
        "contextual data converted into numerical values ----\n",
        "\n",
        "Lable ,ordinal ,onehot encoder,target encoder,mean encoder\n",
        "if u want to give labels go with label encoder\n",
        "if u want to give rank go with ordinal encoder\n",
        "If you will not convert below are the challenges like less accuracy ,we may ignore important fields\n",
        "more columns more accuracy less columns less accuracy\n",
        "Scaling is not there if you will not converting most of the ML akgs involves numerical data\n",
        "\n",
        "one_hpt encoder\n",
        "ordinal encoder ----is two type 1.Label encoder 2.target pr mean encoder\n",
        "in ordinal encoding we will give the ranks or oder manually\n",
        "in label encoding it will give automatically\n",
        "one_hot encoding is used in ML,DL and NLP also\n",
        "meaning ---like colurs red,blue and green if we convert it into table\n",
        "\n",
        "      RED  Blue  GREEN\n",
        "RED    1   0      0\n",
        "Blue   0   1      0\n",
        "GREEN  0   0      1\n",
        "\n",
        "if we have unique category will go with one_hot\n",
        "in one hot encoding will get n-1 columns\n",
        "if we have 100 categories and 100000 records because of one hot encoding it will add extra  another 100 -1 columns to dataframe\n",
        "and it will become complex and the dimensions will increase\n",
        "Target encoding based on the mean of the categorical values will give rank\n",
        "Target encoding reduces dimensionaility\n",
        "\n",
        "      RED  Blue  GREEN\n",
        "RED    1   0      0\n",
        "Blue   0   1      0\n",
        "GREEN  0   0      1\n",
        "RED    1   0      0----in this case will go with frequency encoder\n",
        "\n",
        "Nominal encoder ----three types 1.one hot encoder 2.mean encoder 3.one_hot encoding muiple categories\n",
        "\n",
        "binary encoder\n",
        "frequency encoder\n",
        "target pr mean encoder\n",
        "hash encoder\n",
        "\n",
        "\n",
        "Encoder is techinique used in feature engg---how to choose and scale features and how to convert into numerical data\n",
        "In  leinear regression alg if the data is numerical then only we can draw a line other wise we have to convert the data into numerical values\n",
        "using encoder techniques\n"
      ],
      "metadata": {
        "id": "rZ9dTqwazjQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Titanic ship survived problem  to check what are the columns having non numeric data\n",
        "titanic_exclude_numeric=data.select.dtypes(exclude=np.number).columns\n",
        "titanic_exclude_numeric\n",
        "data['Embarked']\n",
        "data['Embarked'].unique#will give unique values Embarked column\n",
        "\n",
        "ordinal_encoder=OrdinalEncoder(categories=[[1,2,3]])\n",
        "data['Pclass']=ordinal_encoder.fit_transform(data[['Pclass']])\n",
        "data['Pclass']\n",
        "data\n",
        "ordinal_encoder.fit_transform(data[['Pclass']])\n",
        "missing values that is the reason will get float values"
      ],
      "metadata": {
        "id": "60v-CRySzjN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18-10-2024---Encoding Techniques ----ML Model export --LM studio or hugging face --we can download the models\n",
        "'''\n",
        "mode()[0]---means in Embarked column which category is having more values like S or C or Q\n",
        "\n",
        "mode()[1]---means next category having more frequesncy after mode([0])\n",
        "Embarked boolean values are automatically converted into numerical values\n",
        ".pkl file is in binary format..We can store this file in huggingfaxe nevironment\n",
        "wb means write binary and rb means read binary in pickel file\n",
        "For Ml models we can export and import models using pickle or joblib it , in DL we need pytorch tarnsformers\n",
        "Joblib i suitable for large data sets and picle is slower and less memory efficient\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "tjHHmL5HzjLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20-11-2024---Session ----Based on the age predicting height ---using Decission Tree Regressor\n",
        "\n",
        "Classification---o/p is Categories or classes  ---Accuracy they will use\n",
        "Regression ---o/p is conitinuous values ---R2score,MAE,MSE\n",
        "here we have only trwo fields one i/p and one o/p  no need to worry abt root and decission and leaf nodes\n",
        "\n",
        "Reshape the arrays\n",
        "\n",
        "While visualizing the graph if you get error change X_train into X_train.values in because we used drop so\n",
        "the data is in series format .change it to values list format in segregating the X and Y values by using .vaues\n",
        "X=Height_dataset,drop('Height',axis=1).values\n",
        "Y=Height_dataset['Height'].values\n",
        "\n",
        "Run the below command before and after reshape and check the diff\n",
        "print(X_val)\n",
        "\n",
        "If the same data having another column along with Age,Height and Age_Group like from 10 to 30 Grp1\n",
        "30 to 60 Grp2 and 60 to 80 Grp3 here we can use classification algs\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "IYZhnw7ozjIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22-11-2024---Session ----Based on the age predicting height ---using Naive Bays Alg\n",
        "'''\n",
        "It used conditional probability   ---like probability of picking diamond king card from pack of cards\n",
        "In naive bays alg features are independent of each other we can also call these predictors\n",
        "it refers individual features and predicts\n",
        "it is one of probabilistic naive bays classiffier algs assuming independent features\n",
        "used in binary classification and multiclass classification\n",
        "P(A/B) --Means P(A) given that probability event B Occurs\n",
        "X=Age,Chest Pain,Cholestral,Gender ---P(Y/X)----Here X are independent or i/p cariables and Y is o/p\n",
        "Each and every predictors are not dependent ones\n",
        "\n",
        "P(Y/X)=P(X/Y)*P(Y)/P(X)\n",
        "X1=Age X2=Gender X3=Xhest Pain type\n",
        "P(Y/(X1,X2,X3))=P(x1,x2,x3)*P(y)/P(x1,x2,x3)==P(x1/y)*P(x2/y)*P(x3/y)*P(y)/P(x1)*P(x2)*P(x3)\n",
        "\n",
        "Naive--- means assumption--\n",
        "\n",
        "P(Hear Diseae)/{Age,Cholstral}=we can write as P(H D(Yes))/{Age,Cholestral}\n",
        "P(H D(NO))\\{Age,Cholstral}=we can write as P(H D(No))/{Age,Cholestral}\n",
        "\n",
        "P(Yes))\\{Age,Cholestral}=P(Age/Yes)*P(Cholestral/Yes)*P(Yes)/P(Age)*P(Cholestral)\n",
        "\n",
        "P(No))\\{Age,Cholestral}=P(Age/No)*P(Cholestral/No)*P(No)/P(Age)*P(Cholestral)\n",
        "\n",
        "\n",
        "for column in dataset.iloc[:,1:4]:\n",
        "    encoder=LabelEncoder()\n",
        "    dataset[column]=encoder.fit_transform(dataset[column])\n",
        "    label_encoders[column]=encoder\n",
        "label_encoders\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kzSrB1c2zjFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25-11-2024 ---Session----\n",
        "'''\n",
        "In heart.csv file some of the columns are numerical and some categorical values\n",
        "ML Model deployment to Web App---\n",
        "\n",
        "# Scale Numerical Features\n",
        "scaler=StandardScaler()\n",
        "numerical_features=[]\n",
        "for col in data.columns:\n",
        "  if data[col].dtype != 'object': or 'O'\n",
        "    numerical_features.append(col)\n",
        "#numerical_features=numerical_features\n",
        "numerical_features\n",
        "data[numerical_features]=scaler.fit_transform(data[numerical_features])\n",
        "\n",
        "Why are we saving the model,scalar,and encoders ,because we are giving values in the form ,some features we arfe\n",
        "performimg scaling and some them encoding ,ten only it will understand machine understandable format\n",
        "We trained the model for numerical data .that is the reason we saved all the encoders,scalar,and model\n",
        "in Falsk framwork will use these scaling and encoding techniques\n",
        "\n",
        "Creating WebApp using FlaskFramework\n",
        "\n",
        "Copy and paste all the .pkl files into heart disease prediction folder\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "dE6TUH0JzjCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qybUgCzzzi_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esj17T9Wzi8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SuFSOpsJzi54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZQ_Nycpzi25"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}